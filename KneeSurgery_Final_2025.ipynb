{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "rn6otUX29XYN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhcXhA7r9TYz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def rename_columns(df):\n",
        "    # Patient Characteristics columns\n",
        "    patient_characteristics = [\n",
        "        \"Patient number\", \"Age (Continuos)\", \"Sex (Male/female)\", \"BMI (Continuous\",\n",
        "        \"Roken (Yes/No)\", \"Diabetes (Yes/No)\", \"Cardiovascular disease (Yes/No)\",\n",
        "        \"Rheumatoid Arthritis(Yes/No)\"\n",
        "    ]\n",
        "\n",
        "    # Fracture Characteristics columns\n",
        "    fracture_characteristics = [\n",
        "        \"Treatment (Conservative/Operative)\",\n",
        "        \"High Energy trauma (Yes/No)\",\n",
        "        \"Trauma mechanism (Valgus-flexion, Valgus-extension, Valgus-hyperextension, Varus-flexion, Varus-extension, \"\n",
        "        \"Varus-Hyperextension)\",\n",
        "        \"Side Fracture fractuur (Right/Left)\", \"AO/OTA Classification ( 1-6)\",\n",
        "        \"Posterior involvement (Yes/No)\", \"Fracture fibula head (Yes/No)\",\n",
        "        \"Cruris Fracture (Yes/No)\", \"2D gap (Continuous, mm)\", \"2D Step-off (Continuous, mm)\", \"3DGap Area\",\n",
        "        \"AO/OTA - Severtity\"\n",
        "    ]\n",
        "\n",
        "    # Postop Characteristics columns\n",
        "    postop_characteristics = [\n",
        "        \"Condylar width (Continuous, mm)\", \"Incongruence (Continuous, mm)\",\n",
        "        \"MPTA(Continuous, degree)\", \"PPTA (Continuous, degree)\",\n",
        "        \"Revision surgery (Yes/No)\", \"Complicaties (Yes/No)\", \"MPTA (1=wrong,0=good)\",\n",
        "        \"PPTA  (1=wrong,0=good)\"\n",
        "    ]\n",
        "\n",
        "    # Outcome Values columns\n",
        "    outcome_values = [\n",
        "        \"Total Knee Prosthesis within 2 yrs (Yes/No)\", \"Total Knee Prosthesis within 10 yrs (Yes/No)\",\n",
        "        \"Symptoms (Continuous, 0-100)\", \"Pain (Continuous, 0-100)\", \"ADL (Continuous, 0-100)\",\n",
        "        \"Sports (Continuous, 0-100)\", \"Quality of Life (Continuous, 0-100)\", \"VAS-Satisfaction (1-10) \",\n",
        "        \"Total Knee Prosthesis within 5 yrs (Yes/No)\"\n",
        "    ]\n",
        "\n",
        "    raw_column_names = df.iloc[0]\n",
        "    new_column_names = raw_column_names\n",
        "    for i, column in enumerate(raw_column_names):\n",
        "        new_column = \"\"\n",
        "\n",
        "        if column in patient_characteristics:\n",
        "            new_column = \"patient_\" + column\n",
        "        elif column in fracture_characteristics:\n",
        "            new_column = \"fracture_\" + column\n",
        "        elif column in postop_characteristics:\n",
        "            new_column = \"postop_\" + column\n",
        "        elif column in outcome_values:\n",
        "            new_column = \"outcome_\" + column\n",
        "        else:\n",
        "            raise ValueError(f\"Found unknown column in dataset: {column}, have the column names been modified \"\n",
        "                             f\"or have new columns been added?\")\n",
        "\n",
        "        new_column_names[i] = new_column\n",
        "\n",
        "    df.columns = new_column_names\n",
        "    df = df.tail(-1)\n",
        "    return df\n",
        "\n",
        "\n",
        "def update_data_types(df):\n",
        "    type_changes = {\n",
        "        \"patient_Age (Continuos)\": \"float64\",\n",
        "        \"patient_Sex (Male/female)\": \"category\",\n",
        "        \"patient_BMI (Continuous\": \"float64\",\n",
        "        \"fracture_Treatment (Conservative/Operative)\": \"category\",\n",
        "        \"fracture_Trauma mechanism (Valgus-flexion, Valgus-extension, Valgus-hyperextension, Varus-flexion, Varus-extension, Varus-Hyperextension)\": \"category\",\n",
        "        \"fracture_Side Fracture fractuur (Right/Left)\": \"category\",\n",
        "        \"fracture_AO/OTA Classification ( 1-6)\": \"category\",\n",
        "        \"fracture_AO/OTA - Severtity\": \"category\",\n",
        "        \"fracture_2D gap (Continuous, mm)\": \"float64\",\n",
        "        \"fracture_2D Step-off (Continuous, mm)\": \"float64\",\n",
        "        \"fracture_3DGap Area\": \"float64\",\n",
        "        \"postop_Condylar width (Continuous, mm)\": \"float64\",\n",
        "        \"postop_Incongruence (Continuous, mm)\": \"float64\",\n",
        "        \"postop_MPTA(Continuous, degree)\": \"float64\",\n",
        "        \"postop_PPTA (Continuous, degree)\": \"float64\",\n",
        "        \"postop_MPTA (1=wrong,0=good)\":\"bool\",\n",
        "        \"postop_PPTA  (1=wrong,0=good)\":\"bool\",\n",
        "        \"patient_Roken\": \"bool\",\n",
        "        \"patient_Diabetes\": \"bool\",\n",
        "        \"patient_Cardiovascular_Disease\": \"bool\",\n",
        "        \"patient_Rheumatoid_Arthritis\": \"bool\",\n",
        "        \"fracture_High_Energy_trauma\": \"bool\",\n",
        "        \"postop_Revision_Surgery\": \"bool\",\n",
        "        \"outcome_Total_Knee_Prothesis_Within_2_Yrs\": \"bool\",\n",
        "        \"outcome_Total_Knee_Prothesis_Within_5_Yrs\": \"bool\",\n",
        "        \"outcome_Total_Knee_Prothesis_Within_10_Yrs\": \"bool\"\n",
        "    }\n",
        "\n",
        "    for col, dtype in type_changes.items():\n",
        "        if dtype == \"float64\":\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        elif dtype == \"bool\":\n",
        "            df[col] = df[col].astype(\"bool\")\n",
        "        elif dtype == \"category\":\n",
        "            # Attempting to convert to category. If conversion fails, the value will remain as it was.\n",
        "            try:\n",
        "                df[col] = df[col].str.strip()\n",
        "                # Replace multiple spaces with a single space\n",
        "                df[col] = df[col].str.replace(r'\\s+', ' ')\n",
        "                df[col] = df[col].astype('category')\n",
        "            except AttributeError:\n",
        "                pass\n",
        "    return df\n",
        "\n",
        "\n",
        "def update_column(df, column, new_name, func, *args, **kwargs):\n",
        "    df[new_name] = func(df[column], *args, **kwargs)\n",
        "    df.drop(columns=[column], inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def normalize_data(df):\n",
        "    bool_replacement = {\"Yes\": True, \"No\": False, \"no\": False}\n",
        "    replace_func = lambda c, rep_dict: c.replace(rep_dict)\n",
        "\n",
        "    update_column(df, 'patient_Roken (Yes/No)', 'patient_Roken', replace_func,\n",
        "                  bool_replacement)\n",
        "    update_column(df, 'patient_Diabetes (Yes/No)', 'patient_Diabetes', replace_func,\n",
        "                  bool_replacement)\n",
        "    update_column(df, 'patient_Cardiovascular disease (Yes/No)', 'patient_Cardiovascular_Disease',\n",
        "                  replace_func, bool_replacement)\n",
        "    update_column(df, 'patient_Rheumatoid Arthritis(Yes/No)', 'patient_Rheumatoid_Arthritis',\n",
        "                  replace_func, bool_replacement)\n",
        "\n",
        "    update_column(df, 'fracture_High Energy trauma (Yes/No)', 'fracture_High_Energy_trauma',\n",
        "                  replace_func, bool_replacement)\n",
        "    update_column(df, 'fracture_Posterior involvement (Yes/No)', 'fracture_Posterior_Involvement',\n",
        "                  replace_func, bool_replacement)\n",
        "    update_column(df, 'fracture_Fracture fibula head (Yes/No)', 'fracture_Fracture_Fibula_Head',\n",
        "                  replace_func, bool_replacement)\n",
        "    update_column(df, 'fracture_Cruris Fracture (Yes/No)', 'fracture_Cruris_Fracture',\n",
        "                  replace_func, bool_replacement)\n",
        "\n",
        "    update_column(df, 'postop_Revision surgery (Yes/No)', 'postop_Revision_Surgery',\n",
        "                  replace_func, bool_replacement)\n",
        "    update_column(df, 'postop_Complicaties (Yes/No)', 'postop_Complicaties', replace_func,bool_replacement)\n",
        "\n",
        "    update_column(df, 'outcome_Total Knee Prosthesis within 2 yrs (Yes/No)',\n",
        "                  'outcome_Total_Knee_Prothesis_Within_2_Yrs', replace_func,bool_replacement)\n",
        "    update_column(df, 'outcome_Total Knee Prosthesis within 5 yrs (Yes/No)',\n",
        "                  'outcome_Total_Knee_Prothesis_Within_5_Yrs', replace_func,bool_replacement)\n",
        "    update_column(df, 'outcome_Total Knee Prosthesis within 10 yrs (Yes/No)',\n",
        "                  'outcome_Total_Knee_Prothesis_Within_10_Yrs', replace_func,bool_replacement)\n",
        "\n",
        "    return df\n",
        "\n",
        "def preprocess(data_dir=\"data\", filename=\"Features-Filled-V3.xlsx\"):\n",
        "    filepath = Path(data_dir).joinpath(filename)\n",
        "    df = pd.read_excel(filepath)\n",
        "\n",
        "    df = rename_columns(df)\n",
        "    df = normalize_data(df)\n",
        "    df = update_data_types(df)\n",
        "\n",
        "    preprocessed_path = Path(data_dir).joinpath(\"preprocessed.csv\")\n",
        "    df.to_csv(preprocessed_path, index=False)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    preprocess()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluation"
      ],
      "metadata": {
        "id": "2COyjZt-9ayW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (roc_curve, auc, f1_score, confusion_matrix,\n",
        "                             precision_score, recall_score, ConfusionMatrixDisplay)\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pickle\n",
        "from patsy import dmatrix\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'preprocessed.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "center_column = 'patient_Hospital'  # Adjust as needed\n",
        "unique_centers = df[center_column].unique()\n",
        "\n",
        "# Define features\n",
        "continuous_features = [\n",
        "    \"fracture_2D gap (Continuous, mm)\",\n",
        "    \"fracture_2D Step-off (Continuous, mm)\",\n",
        "    \"patient_Age (Continuos)\",\n",
        "    \"patient_BMI (Continuous\"\n",
        "]\n",
        "\n",
        "categorical_features = [\n",
        "    \"patient_Sex (Male/female)\",\n",
        "    \"fracture_AO/OTA Classification ( 1-6)\",\n",
        "    \"postop_MPTA (1=wrong,0=good)\",\n",
        "    \"postop_PPTA  (1=wrong,0=good)\"\n",
        "]\n",
        "\n",
        "# Create an 'output' folder (at the top level) if it doesn't exist\n",
        "os.makedirs(\"output\", exist_ok=True)\n",
        "\n",
        "# We'll store outcomes.csv here\n",
        "outcomes_path = os.path.join(\"output\", \"outcomes.csv\")\n",
        "\n",
        "years = [2, 5]\n",
        "models = ['logistic', 'forest', 'xgboost']\n",
        "\n",
        "for year in years:\n",
        "    target = f\"outcome_Total_Knee_Prothesis_Within_{year}_Yrs\"\n",
        "\n",
        "    # Drop rows with missing target\n",
        "    df_year = df.dropna(subset=[target])\n",
        "    y = df_year[target].astype(bool)\n",
        "\n",
        "    for model_name in models:\n",
        "        print(f\"\\nModel: {model_name}, Year: {year}\")\n",
        "\n",
        "        # --------------------------------------------------\n",
        "        # Create a subfolder inside 'output' for this run\n",
        "        # e.g. \"output/logistic_2\"\n",
        "        # --------------------------------------------------\n",
        "        run_folder = os.path.join(\"output\", f\"{model_name}_{year}\")\n",
        "        os.makedirs(run_folder, exist_ok=True)\n",
        "\n",
        "        # Lists for metrics\n",
        "        all_f1_scores = []\n",
        "        all_aucs = []\n",
        "        all_precision = []\n",
        "        all_recall = []\n",
        "\n",
        "        # -- NEW: For macro metrics\n",
        "        all_f1_macro = []\n",
        "        all_precision_macro = []\n",
        "        all_recall_macro = []\n",
        "\n",
        "        cumulative_confusion_matrix = np.zeros((2, 2))\n",
        "        all_probs = []\n",
        "        all_y_test = []\n",
        "        all_y_pred = []\n",
        "\n",
        "        # We'll collect patient-level predictions here,\n",
        "        # then write them out to predictions.csv at the end of this model-year loop.\n",
        "        predictions_list_for_model_year = []\n",
        "\n",
        "        # -- NEW: We'll collect feature importances (per fold) here\n",
        "        feature_importances_list_for_model_year = []\n",
        "\n",
        "        # -----------------------------------------------------------------\n",
        "        # ADDED: Lists to store each fold's ROC for the multi‐fold plot\n",
        "        # -----------------------------------------------------------------\n",
        "        fold_fprs = []\n",
        "        fold_tprs = []\n",
        "        fold_roc_aucs = []\n",
        "\n",
        "        # External validation by leaving one center out\n",
        "        for test_center in unique_centers:\n",
        "            train_mask = df_year[center_column] != test_center\n",
        "            test_mask  = df_year[center_column] == test_center\n",
        "\n",
        "            # Split data\n",
        "            X_train_df, X_test_df = df_year.loc[train_mask], df_year.loc[test_mask]\n",
        "            y_train, y_test       = y[train_mask], y[test_mask]\n",
        "\n",
        "            if len(y_test) == 0:\n",
        "                continue\n",
        "\n",
        "            # -------------------------------------------------\n",
        "            # 1. Split continuous vs. categorical in training\n",
        "            # -------------------------------------------------\n",
        "            X_train_cont = X_train_df[continuous_features].copy()\n",
        "            X_test_cont  = X_test_df[continuous_features].copy()\n",
        "\n",
        "            X_train_cat = X_train_df[categorical_features].copy()\n",
        "            X_test_cat  = X_test_df[categorical_features].copy()\n",
        "\n",
        "            # -------------------------------------------------\n",
        "            # 2. Impute missing in continuous, then scale\n",
        "            #    (fit only on training)\n",
        "            # -------------------------------------------------\n",
        "            imputer = KNNImputer(n_neighbors=5)\n",
        "            X_train_cont_imputed = imputer.fit_transform(X_train_cont)\n",
        "            X_test_cont_imputed  = imputer.transform(X_test_cont)\n",
        "\n",
        "            scaler = StandardScaler()\n",
        "            X_train_cont_scaled = scaler.fit_transform(X_train_cont_imputed)\n",
        "            X_test_cont_scaled  = scaler.transform(X_test_cont_imputed)\n",
        "\n",
        "            # Turn back into DataFrames so we can reference columns\n",
        "            X_train_cont_scaled = pd.DataFrame(X_train_cont_scaled,\n",
        "                                               columns=continuous_features,\n",
        "                                               index=X_train_df.index)\n",
        "            X_test_cont_scaled = pd.DataFrame(X_test_cont_scaled,\n",
        "                                              columns=continuous_features,\n",
        "                                              index=X_test_df.index)\n",
        "\n",
        "            if model_name == 'logistic':\n",
        "                # -------------------------------------------------\n",
        "                # 3. Create splines from *scaled* continuous data\n",
        "                # -------------------------------------------------\n",
        "                X_spline_list_train = []\n",
        "                X_spline_list_test = []\n",
        "\n",
        "                # We'll track the features actually used in splines\n",
        "                used_spline_feats = []\n",
        "\n",
        "                for cfeat in continuous_features:\n",
        "                    cvals_train = X_train_cont_scaled[cfeat].dropna()\n",
        "                    # If almost no data remains, skip feature\n",
        "                    if len(cvals_train) < 4:\n",
        "                        print(f\"Dropping {cfeat} due to insufficient data.\")\n",
        "                        continue\n",
        "\n",
        "                    # Generate knots from scaled training\n",
        "                    knots = np.percentile(cvals_train, [25, 50, 75])\n",
        "\n",
        "                    # Create spline basis for training set\n",
        "                    spline_train = dmatrix(\n",
        "                        \"bs(x, knots=({},{},{}), degree=3, include_intercept=False)\".format(*knots),\n",
        "                        {\"x\": X_train_cont_scaled[cfeat]},\n",
        "                        return_type='dataframe'\n",
        "                    )\n",
        "                    spline_col_names = [f\"{cfeat}_spline_{i}\" for i in range(spline_train.shape[1])]\n",
        "                    spline_train.columns = spline_col_names\n",
        "                    X_spline_list_train.append(spline_train)\n",
        "\n",
        "                    # Replicate same spline transformations in test\n",
        "                    spline_test = dmatrix(\n",
        "                        \"bs(x, knots=({},{},{}), degree=3, include_intercept=False)\".format(*knots),\n",
        "                        {\"x\": X_test_cont_scaled[cfeat]},\n",
        "                        return_type='dataframe'\n",
        "                    )\n",
        "                    spline_test.columns = spline_col_names\n",
        "                    X_spline_list_test.append(spline_test)\n",
        "\n",
        "                    used_spline_feats.append(cfeat)\n",
        "\n",
        "                # Combine all spline features\n",
        "                if len(X_spline_list_train) > 0:\n",
        "                    X_spline_all_train = pd.concat(X_spline_list_train, axis=1)\n",
        "                    X_spline_all_test  = pd.concat(X_spline_list_test, axis=1)\n",
        "                else:\n",
        "                    # If no valid spline features remain\n",
        "                    X_spline_all_train = pd.DataFrame(index=X_train_df.index)\n",
        "                    X_spline_all_test  = pd.DataFrame(index=X_test_df.index)\n",
        "\n",
        "                # -------------------------------------------------\n",
        "                # 4. Combine (scaled) spline features with cat\n",
        "                # -------------------------------------------------\n",
        "                X_train_final = pd.concat([X_train_cat, X_spline_all_train], axis=1)\n",
        "                X_test_final  = pd.concat([X_test_cat,  X_spline_all_test],  axis=1)\n",
        "\n",
        "            else:\n",
        "                # For non-logistic, we just use the scaled numeric + cat\n",
        "                X_train_final = pd.concat([X_train_cont_scaled, X_train_cat], axis=1)\n",
        "                X_test_final  = pd.concat([X_test_cont_scaled,  X_test_cat],  axis=1)\n",
        "\n",
        "            print(f\"train_columns: {X_train_final.columns}\", len(X_train_final.columns))\n",
        "\n",
        "            # -------------------------------------------------\n",
        "            # 5. Now we have X_train_final, X_test_final ready\n",
        "            #    Train model, do threshold search, evaluate\n",
        "            # -------------------------------------------------\n",
        "            # Model selection\n",
        "            if model_name == 'logistic':\n",
        "                base_model = LogisticRegression()\n",
        "            elif model_name == 'forest':\n",
        "                base_model = RandomForestClassifier()\n",
        "            elif model_name == 'xgboost':\n",
        "                base_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "            # Nested validation for threshold selection\n",
        "            X_train_nested, X_val_nested, y_train_nested, y_val_nested = train_test_split(\n",
        "                X_train_final, y_train, test_size=0.2, random_state=42\n",
        "            )\n",
        "            model = base_model\n",
        "            model.fit(X_train_nested, y_train_nested)\n",
        "\n",
        "            thresholds = np.linspace(0.1, 0.9, 9)\n",
        "            y_proba_val = model.predict_proba(X_val_nested)[:, 1]\n",
        "            best_threshold = 0\n",
        "            best_f1 = 0\n",
        "            for threshold in thresholds:\n",
        "                y_pred_threshold = (y_proba_val >= threshold).astype(int)\n",
        "                f1_tmp = f1_score(y_val_nested, y_pred_threshold)\n",
        "                if f1_tmp > best_f1:\n",
        "                    best_f1 = f1_tmp\n",
        "                    best_threshold = threshold\n",
        "\n",
        "            # Retrain on full training set\n",
        "            model.fit(X_train_final, y_train)\n",
        "            y_proba = model.predict_proba(X_test_final)[:, 1]\n",
        "            y_pred = (y_proba >= best_threshold).astype(int)\n",
        "\n",
        "            # Metrics (binary average)\n",
        "            f1       = f1_score(y_test, y_pred)\n",
        "            precision= precision_score(y_test, y_pred)\n",
        "            recall   = recall_score(y_test, y_pred)\n",
        "\n",
        "            # -- NEW: macro-averaged metrics\n",
        "            f1_macro       = f1_score(y_test, y_pred, average='macro')\n",
        "            precision_macro= precision_score(y_test, y_pred, average='macro')\n",
        "            recall_macro   = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "            fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "            roc_auc     = auc(fpr, tpr)\n",
        "\n",
        "            all_f1_scores.append(f1)\n",
        "            all_aucs.append(roc_auc)\n",
        "            all_precision.append(precision)\n",
        "            all_recall.append(recall)\n",
        "\n",
        "            all_f1_macro.append(f1_macro)\n",
        "            all_precision_macro.append(precision_macro)\n",
        "            all_recall_macro.append(recall_macro)\n",
        "\n",
        "            all_probs.extend(y_proba)\n",
        "            all_y_test.extend(y_test)\n",
        "            all_y_pred.extend(y_pred)\n",
        "\n",
        "            fold_confusion_matrix = confusion_matrix(y_test, y_pred)\n",
        "            cumulative_confusion_matrix += fold_confusion_matrix\n",
        "\n",
        "            # --------------------------------------------\n",
        "            # Collect patient-level predictions for output\n",
        "            # --------------------------------------------\n",
        "            df_pred_center = pd.DataFrame({\n",
        "                \"patient_Patient number\": X_test_df[\"patient_Patient number\"],  # adapt if needed\n",
        "                \"center\": test_center,\n",
        "                \"year\": year,\n",
        "                \"model\": model_name,\n",
        "                \"true_label\": y_test.values,\n",
        "                \"predicted_proba\": y_proba,\n",
        "                \"predicted_label\": y_pred\n",
        "            })\n",
        "            predictions_list_for_model_year.append(df_pred_center)\n",
        "\n",
        "            # --------------------------------------------\n",
        "            # Collect feature importances for this fold\n",
        "            # --------------------------------------------\n",
        "            if model_name == 'logistic':\n",
        "                importances = model.coef_[0]  # shape: (n_features,)\n",
        "            else:\n",
        "                importances = model.feature_importances_  # shape: (n_features,)\n",
        "\n",
        "            final_feature_names = X_train_final.columns.tolist()\n",
        "\n",
        "            df_importance = pd.DataFrame({\n",
        "                'feature': final_feature_names,\n",
        "                'importance': importances\n",
        "            })\n",
        "            df_importance['center'] = test_center\n",
        "            df_importance['model'] = model_name\n",
        "            df_importance['year'] = year\n",
        "\n",
        "            feature_importances_list_for_model_year.append(df_importance)\n",
        "\n",
        "            # --------------------------------------------\n",
        "            # ADDED: Save fold's ROC data for the final plot\n",
        "            # --------------------------------------------\n",
        "            fold_fprs.append(fpr)\n",
        "            fold_tprs.append(tpr)\n",
        "            fold_roc_aucs.append(roc_auc)\n",
        "\n",
        "        # End of center loop\n",
        "\n",
        "        # -------------------------------\n",
        "        # ADDED: Plot the multi‐fold ROC\n",
        "        # -------------------------------\n",
        "        if len(fold_fprs) > 0:\n",
        "            plt.figure()\n",
        "\n",
        "            # We'll use a matplotlib color map, picking one color per fold.\n",
        "            cmap = plt.cm.get_cmap('viridis')\n",
        "            colors_list = [cmap(i / (len(fold_fprs) - 1))\n",
        "                          for i in range(len(fold_fprs))]\n",
        "\n",
        "            # Plot each fold with its own color & label\n",
        "            for i, (fpr_i, tpr_i) in enumerate(zip(fold_fprs, fold_tprs)):\n",
        "                plt.plot(\n",
        "                    fpr_i,\n",
        "                    tpr_i,\n",
        "                    color=colors_list[i],\n",
        "                    alpha=0.6,  # some transparency so they don't overwhelm\n",
        "                    label=f'Fold {i} (AUC = {fold_roc_aucs[i]:.2f})'\n",
        "                )\n",
        "\n",
        "            # Interpolate all TPRs to a common FPR axis\n",
        "            mean_fpr = np.linspace(0, 1, 100)\n",
        "            tprs_interp = []\n",
        "            for fpr_i, tpr_i in zip(fold_fprs, fold_tprs):\n",
        "                tpr_interp = np.interp(mean_fpr, fpr_i, tpr_i)\n",
        "                tpr_interp[0] = 0.0\n",
        "                tprs_interp.append(tpr_interp)\n",
        "\n",
        "            mean_tpr = np.mean(tprs_interp, axis=0)\n",
        "            std_tpr = np.std(tprs_interp, axis=0)\n",
        "            mean_auc_folds = np.mean(fold_roc_aucs)\n",
        "            std_auc_folds  = np.std(fold_roc_aucs)\n",
        "\n",
        "            # Plot mean ROC in dark blue\n",
        "            plt.plot(\n",
        "                mean_fpr,\n",
        "                mean_tpr,\n",
        "                color='blue',\n",
        "                linewidth=2,\n",
        "                label=r'Mean ROC (AUC = %.2f $\\pm$ %.2f)' % (mean_auc_folds, std_auc_folds)\n",
        "            )\n",
        "\n",
        "            # Shade ±1 std. dev. around the mean\n",
        "            plt.fill_between(\n",
        "                mean_fpr,\n",
        "                mean_tpr - std_tpr,\n",
        "                mean_tpr + std_tpr,\n",
        "                color='grey',\n",
        "                alpha=0.2,\n",
        "                label=r'$\\pm$ 1 std. dev.'\n",
        "            )\n",
        "\n",
        "            # Chance line (red, dashed)\n",
        "            plt.plot([0, 1], [0, 1], linestyle='--', color='red', label='Chance')\n",
        "\n",
        "            plt.xlabel('False Positive Rate (Positive label: True)')\n",
        "            plt.ylabel('True Positive Rate (Positive label: True)')\n",
        "            plt.title('Receiver Operating Characteristic')\n",
        "            plt.legend(loc='lower right')\n",
        "\n",
        "            # Save the figure to the current model-year folder\n",
        "            plt.savefig(os.path.join(run_folder, \"roc_curves.png\"))\n",
        "            plt.show()\n",
        "\n",
        "        # ---------------------------------------------\n",
        "        # Print average results across centers (existing code)\n",
        "        # ---------------------------------------------\n",
        "        if len(all_f1_scores) > 0:\n",
        "            mean_f1       = np.mean(all_f1_scores)\n",
        "            mean_auc      = np.mean(all_aucs)\n",
        "            mean_precision= np.mean(all_precision)\n",
        "            mean_recall   = np.mean(all_recall)\n",
        "\n",
        "            mean_f1_macro       = np.mean(all_f1_macro)\n",
        "            mean_precision_macro= np.mean(all_precision_macro)\n",
        "            mean_recall_macro   = np.mean(all_recall_macro)\n",
        "\n",
        "            print(f\"Mean F1: {mean_f1:.2f}\")\n",
        "            print(f\"Mean AUC: {mean_auc:.2f}\")\n",
        "            print(f\"Mean Precision: {mean_precision:.2f}\")\n",
        "            print(f\"Mean Recall: {mean_recall:.2f}\")\n",
        "\n",
        "            # NEW: print macro metrics\n",
        "            print(f\"Mean F1 (macro): {mean_f1_macro:.2f}\")\n",
        "            print(f\"Mean Precision (macro): {mean_precision_macro:.2f}\")\n",
        "            print(f\"Mean Recall (macro): {mean_recall_macro:.2f}\")\n",
        "\n",
        "            # -----------------------------------------\n",
        "            # Confusion matrix & store in local folder\n",
        "            # -----------------------------------------\n",
        "            ConfusionMatrixDisplay(confusion_matrix=cumulative_confusion_matrix).plot(values_format='g')\n",
        "            plt.title(f\"Confusion Matrix: {model_name}, {year}-Year\")\n",
        "            plt.savefig(os.path.join(run_folder, \"confusion_matrix.png\"))\n",
        "            plt.show()\n",
        "\n",
        "            # Bootstrapping for AUC variability\n",
        "            n_bootstraps = 100\n",
        "            rng = np.random.RandomState(42)\n",
        "            boot_aucs = []\n",
        "            all_y_test_arr = np.array(all_y_test)\n",
        "            all_probs_arr  = np.array(all_probs)\n",
        "            for i in range(n_bootstraps):\n",
        "                indices = rng.randint(0, len(all_y_test_arr), len(all_y_test_arr))\n",
        "                # If the sample is all one class, skip\n",
        "                if len(np.unique(all_y_test_arr[indices])) < 2:\n",
        "                    continue\n",
        "                fpr_b, tpr_b, _ = roc_curve(all_y_test_arr[indices], all_probs_arr[indices])\n",
        "                roc_auc_b = auc(fpr_b, tpr_b)\n",
        "                boot_aucs.append(roc_auc_b)\n",
        "\n",
        "            if len(boot_aucs) > 0:\n",
        "                print(f\"Bootstrap AUC Mean: {np.mean(boot_aucs):.2f}, \"\n",
        "                      f\"95% CI: ({np.percentile(boot_aucs,2.5):.2f}-{np.percentile(boot_aucs,97.5):.2f})\")\n",
        "\n",
        "            # -------------------------------------------------\n",
        "            # Calibration curve & store in local model folder\n",
        "            # -------------------------------------------------\n",
        "            prob_true, prob_pred = calibration_curve(all_y_test, all_probs, n_bins=10)\n",
        "            plt.figure()\n",
        "            plt.plot(prob_pred, prob_true, marker='o', label='Calibration')\n",
        "            plt.plot([0,1],[0,1], 'k--', label='Perfectly calibrated')\n",
        "            plt.title(f\"Calibration Curve: {model_name}, {year}-Year\")\n",
        "            plt.xlabel('Predicted Probability')\n",
        "            plt.ylabel('Fraction of Positives')\n",
        "            plt.legend()\n",
        "            plt.savefig(os.path.join(run_folder, \"calibration_curve.png\"))\n",
        "            plt.show()\n",
        "\n",
        "            # Summarize run data (for outcomes.csv in 'output' folder)\n",
        "            run_data = {\n",
        "                \"name\": f\"{model_name}_{year}\",\n",
        "                \"auc\": round(mean_auc, 3),\n",
        "                \"f1\": round(mean_f1, 3),\n",
        "                \"recall\": round(mean_recall, 3),\n",
        "                \"precision\": round(mean_precision, 3),\n",
        "                # NEW: macro-averaged metrics\n",
        "                \"f1_macro\": round(mean_f1_macro, 3),\n",
        "                \"recall_macro\": round(mean_recall_macro, 3),\n",
        "                \"precision_macro\": round(mean_precision_macro, 3),\n",
        "\n",
        "                \"bootstrap_auc_mean\": round(np.mean(boot_aucs), 3) if len(boot_aucs) > 0 else np.nan,\n",
        "                \"bootstrap_auc_ci_lower\": round(np.percentile(boot_aucs, 2.5), 3) if len(boot_aucs) > 0 else np.nan,\n",
        "                \"bootstrap_auc_ci_upper\": round(np.percentile(boot_aucs, 97.5), 3) if len(boot_aucs) > 0 else np.nan,\n",
        "            }\n",
        "\n",
        "            # Append results to outcomes.csv in 'output/'\n",
        "            try:\n",
        "                outcome = pd.read_csv(outcomes_path)\n",
        "                outcome = pd.concat([outcome, pd.DataFrame.from_dict([run_data])], ignore_index=True)\n",
        "                outcome.to_csv(outcomes_path, index=False)\n",
        "            except FileNotFoundError:\n",
        "                pd.DataFrame.from_dict([run_data]).to_csv(outcomes_path, index=False)\n",
        "\n",
        "            # --------------------------------------------\n",
        "            # Store predictions for this model+year folder\n",
        "            # --------------------------------------------\n",
        "            if len(predictions_list_for_model_year) > 0:\n",
        "                df_all_preds_this_model_year = pd.concat(predictions_list_for_model_year, ignore_index=True)\n",
        "                df_all_preds_this_model_year.to_csv(\n",
        "                    os.path.join(run_folder, \"predictions.csv\"),\n",
        "                    index=False\n",
        "                )\n",
        "\n",
        "            # -------------------------------------------------------\n",
        "            # Store feature importances across folds in this run folder\n",
        "            # -------------------------------------------------------\n",
        "            if len(feature_importances_list_for_model_year) > 0:\n",
        "                df_feature_importances = pd.concat(feature_importances_list_for_model_year, ignore_index=True)\n",
        "                df_feature_importances.to_csv(\n",
        "                    os.path.join(run_folder, \"feature_importances.csv\"),\n",
        "                    index=False\n",
        "                )\n",
        "\n",
        "            # ------------------------------------------------\n",
        "            # Always save model & scaler in current run folder\n",
        "            # ------------------------------------------------\n",
        "            model_filename = os.path.join(run_folder, \"model.sav\")\n",
        "            pickle.dump(model, open(model_filename, 'wb'))\n",
        "\n",
        "            # Save the scaler (only relevant to continuous features)\n",
        "            scaler_filename = os.path.join(run_folder, \"scaler.sav\")\n",
        "            pickle.dump(scaler, open(scaler_filename, 'wb'))\n"
      ],
      "metadata": {
        "id": "0fwHQEmS9c_U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}