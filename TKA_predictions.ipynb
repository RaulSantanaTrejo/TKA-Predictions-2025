{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "rn6otUX29XYN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhcXhA7r9TYz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def rename_columns(df):\n",
        "    # Patient Characteristics columns\n",
        "    patient_characteristics = [\n",
        "        \"Patient number\", \"Age (Continuos)\", \"Sex (Male/female)\", \"BMI (Continuous\",\n",
        "        \"Roken (Yes/No)\", \"Diabetes (Yes/No)\", \"Cardiovascular disease (Yes/No)\",\n",
        "        \"Rheumatoid Arthritis(Yes/No)\"\n",
        "    ]\n",
        "\n",
        "    # Fracture Characteristics columns\n",
        "    fracture_characteristics = [\n",
        "        \"Treatment (Conservative/Operative)\",\n",
        "        \"High Energy trauma (Yes/No)\",\n",
        "        \"Trauma mechanism (Valgus-flexion, Valgus-extension, Valgus-hyperextension, Varus-flexion, Varus-extension, \"\n",
        "        \"Varus-Hyperextension)\",\n",
        "        \"Side Fracture fractuur (Right/Left)\", \"AO/OTA Classification ( 1-6)\",\n",
        "        \"Posterior involvement (Yes/No)\", \"Fracture fibula head (Yes/No)\",\n",
        "        \"Cruris Fracture (Yes/No)\", \"2D gap (Continuous, mm)\", \"2D Step-off (Continuous, mm)\", \"3DGap Area\",\n",
        "        \"AO/OTA - Severtity\"\n",
        "    ]\n",
        "\n",
        "    # Postop Characteristics columns\n",
        "    postop_characteristics = [\n",
        "        \"Condylar width (Continuous, mm)\", \"Incongruence (Continuous, mm)\",\n",
        "        \"MPTA(Continuous, degree)\", \"PPTA (Continuous, degree)\",\n",
        "        \"Revision surgery (Yes/No)\", \"Complicaties (Yes/No)\", \"MPTA (1=wrong,0=good)\",\n",
        "        \"PPTA  (1=wrong,0=good)\"\n",
        "    ]\n",
        "\n",
        "    # Outcome Values columns\n",
        "    outcome_values = [\n",
        "        \"Total Knee Prosthesis within 2 yrs (Yes/No)\", \"Total Knee Prosthesis within 10 yrs (Yes/No)\",\n",
        "        \"Symptoms (Continuous, 0-100)\", \"Pain (Continuous, 0-100)\", \"ADL (Continuous, 0-100)\",\n",
        "        \"Sports (Continuous, 0-100)\", \"Quality of Life (Continuous, 0-100)\", \"VAS-Satisfaction (1-10) \",\n",
        "        \"Total Knee Prosthesis within 5 yrs (Yes/No)\"\n",
        "    ]\n",
        "\n",
        "    raw_column_names = df.iloc[0]\n",
        "    new_column_names = raw_column_names\n",
        "    for i, column in enumerate(raw_column_names):\n",
        "        new_column = \"\"\n",
        "\n",
        "        if column in patient_characteristics:\n",
        "            new_column = \"patient_\" + column\n",
        "        elif column in fracture_characteristics:\n",
        "            new_column = \"fracture_\" + column\n",
        "        elif column in postop_characteristics:\n",
        "            new_column = \"postop_\" + column\n",
        "        elif column in outcome_values:\n",
        "            new_column = \"outcome_\" + column\n",
        "        else:\n",
        "            raise ValueError(f\"Found unknown column in dataset: {column}, have the column names been modified \"\n",
        "                             f\"or have new columns been added?\")\n",
        "\n",
        "        new_column_names[i] = new_column\n",
        "\n",
        "    df.columns = new_column_names\n",
        "    df = df.tail(-1)\n",
        "    return df\n",
        "\n",
        "\n",
        "def update_data_types(df):\n",
        "    type_changes = {\n",
        "        \"patient_Age (Continuos)\": \"float64\",\n",
        "        \"patient_Sex (Male/female)\": \"category\",\n",
        "        \"patient_BMI (Continuous\": \"float64\",\n",
        "        \"fracture_Treatment (Conservative/Operative)\": \"category\",\n",
        "        \"fracture_Trauma mechanism (Valgus-flexion, Valgus-extension, Valgus-hyperextension, Varus-flexion, Varus-extension, Varus-Hyperextension)\": \"category\",\n",
        "        \"fracture_Side Fracture fractuur (Right/Left)\": \"category\",\n",
        "        \"fracture_AO/OTA Classification ( 1-6)\": \"category\",\n",
        "        \"fracture_AO/OTA - Severtity\": \"category\",\n",
        "        \"fracture_2D gap (Continuous, mm)\": \"float64\",\n",
        "        \"fracture_2D Step-off (Continuous, mm)\": \"float64\",\n",
        "        \"fracture_3DGap Area\": \"float64\",\n",
        "        \"postop_Condylar width (Continuous, mm)\": \"float64\",\n",
        "        \"postop_Incongruence (Continuous, mm)\": \"float64\",\n",
        "        \"postop_MPTA(Continuous, degree)\": \"float64\",\n",
        "        \"postop_PPTA (Continuous, degree)\": \"float64\",\n",
        "        \"postop_MPTA (1=wrong,0=good)\":\"bool\",\n",
        "        \"postop_PPTA  (1=wrong,0=good)\":\"bool\",\n",
        "        \"patient_Roken\": \"bool\",\n",
        "        \"patient_Diabetes\": \"bool\",\n",
        "        \"patient_Cardiovascular_Disease\": \"bool\",\n",
        "        \"patient_Rheumatoid_Arthritis\": \"bool\",\n",
        "        \"fracture_High_Energy_trauma\": \"bool\",\n",
        "        \"postop_Revision_Surgery\": \"bool\",\n",
        "        \"outcome_Total_Knee_Prothesis_Within_2_Yrs\": \"bool\",\n",
        "        \"outcome_Total_Knee_Prothesis_Within_5_Yrs\": \"bool\",\n",
        "        \"outcome_Total_Knee_Prothesis_Within_10_Yrs\": \"bool\"\n",
        "    }\n",
        "\n",
        "    for col, dtype in type_changes.items():\n",
        "        if dtype == \"float64\":\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        elif dtype == \"bool\":\n",
        "            df[col] = df[col].astype(\"bool\")\n",
        "        elif dtype == \"category\":\n",
        "            # Attempting to convert to category. If conversion fails, the value will remain as it was.\n",
        "            try:\n",
        "                df[col] = df[col].str.strip()\n",
        "                # Replace multiple spaces with a single space\n",
        "                df[col] = df[col].str.replace(r'\\s+', ' ')\n",
        "                df[col] = df[col].astype('category')\n",
        "            except AttributeError:\n",
        "                pass\n",
        "    return df\n",
        "\n",
        "\n",
        "def update_column(df, column, new_name, func, *args, **kwargs):\n",
        "    df[new_name] = func(df[column], *args, **kwargs)\n",
        "    df.drop(columns=[column], inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def normalize_data(df):\n",
        "    bool_replacement = {\"Yes\": True, \"No\": False, \"no\": False}\n",
        "    replace_func = lambda c, rep_dict: c.replace(rep_dict)\n",
        "\n",
        "    update_column(df, 'patient_Roken (Yes/No)', 'patient_Roken', replace_func,\n",
        "                  bool_replacement)\n",
        "    update_column(df, 'patient_Diabetes (Yes/No)', 'patient_Diabetes', replace_func,\n",
        "                  bool_replacement)\n",
        "    update_column(df, 'patient_Cardiovascular disease (Yes/No)', 'patient_Cardiovascular_Disease',\n",
        "                  replace_func, bool_replacement)\n",
        "    update_column(df, 'patient_Rheumatoid Arthritis(Yes/No)', 'patient_Rheumatoid_Arthritis',\n",
        "                  replace_func, bool_replacement)\n",
        "\n",
        "    update_column(df, 'fracture_High Energy trauma (Yes/No)', 'fracture_High_Energy_trauma',\n",
        "                  replace_func, bool_replacement)\n",
        "    update_column(df, 'fracture_Posterior involvement (Yes/No)', 'fracture_Posterior_Involvement',\n",
        "                  replace_func, bool_replacement)\n",
        "    update_column(df, 'fracture_Fracture fibula head (Yes/No)', 'fracture_Fracture_Fibula_Head',\n",
        "                  replace_func, bool_replacement)\n",
        "    update_column(df, 'fracture_Cruris Fracture (Yes/No)', 'fracture_Cruris_Fracture',\n",
        "                  replace_func, bool_replacement)\n",
        "\n",
        "    update_column(df, 'postop_Revision surgery (Yes/No)', 'postop_Revision_Surgery',\n",
        "                  replace_func, bool_replacement)\n",
        "    update_column(df, 'postop_Complicaties (Yes/No)', 'postop_Complicaties', replace_func,bool_replacement)\n",
        "\n",
        "    update_column(df, 'outcome_Total Knee Prosthesis within 2 yrs (Yes/No)',\n",
        "                  'outcome_Total_Knee_Prothesis_Within_2_Yrs', replace_func,bool_replacement)\n",
        "    update_column(df, 'outcome_Total Knee Prosthesis within 5 yrs (Yes/No)',\n",
        "                  'outcome_Total_Knee_Prothesis_Within_5_Yrs', replace_func,bool_replacement)\n",
        "    update_column(df, 'outcome_Total Knee Prosthesis within 10 yrs (Yes/No)',\n",
        "                  'outcome_Total_Knee_Prothesis_Within_10_Yrs', replace_func,bool_replacement)\n",
        "\n",
        "    return df\n",
        "\n",
        "def preprocess(data_dir=\"data\", filename=\"Features-Filled-V3.xlsx\"):\n",
        "    filepath = Path(data_dir).joinpath(filename)\n",
        "    df = pd.read_excel(filepath)\n",
        "\n",
        "    df = rename_columns(df)\n",
        "    df = normalize_data(df)\n",
        "    df = update_data_types(df)\n",
        "\n",
        "    preprocessed_path = Path(data_dir).joinpath(\"preprocessed.csv\")\n",
        "    df.to_csv(preprocessed_path, index=False)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    preprocess()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluation"
      ],
      "metadata": {
        "id": "2COyjZt-9ayW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (roc_curve, auc, f1_score, confusion_matrix,\n",
        "                             precision_score, recall_score, ConfusionMatrixDisplay)\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pickle\n",
        "\n",
        "from patsy.splines import bs  # Direct use of the spline function\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1) INPUT BOUNDARIES (BIOLOGICALLY BACKED)\n",
        "# ------------------------------------------------------------------------------\n",
        "input_bounds = {\n",
        "    \"gap\":      [0, 100],\n",
        "    \"stepoff\":  [0, 100],\n",
        "    \"age\":      [0, 120],\n",
        "    \"bmi\":      [0, 50]\n",
        "}\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2) MAP FROM COLUMN NAMES TO THE KEYS IN 'input_bounds'\n",
        "#    (So we can pick the correct [min, max] for each feature.)\n",
        "# ------------------------------------------------------------------------------\n",
        "feature_bounds_map = {\n",
        "    \"fracture_2D gap (Continuous, mm)\":        \"gap\",\n",
        "    \"fracture_2D Step-off (Continuous, mm)\":   \"stepoff\",\n",
        "    \"patient_Age (Continuos)\":                 \"age\",\n",
        "    \"patient_BMI (Continuous\":                 \"bmi\"  # note the missing parenthesis in the CSV\n",
        "}\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3) LOAD DATA & SETUP\n",
        "# ------------------------------------------------------------------------------\n",
        "file_path = 'preprocessed.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "center_column = 'patient_Hospital'  # Adjust if needed\n",
        "unique_centers = df[center_column].unique()\n",
        "\n",
        "continuous_features = [\n",
        "    \"fracture_2D gap (Continuous, mm)\",\n",
        "    \"fracture_2D Step-off (Continuous, mm)\",\n",
        "    \"patient_Age (Continuos)\",\n",
        "    \"patient_BMI (Continuous\"\n",
        "]\n",
        "\n",
        "categorical_features = [\n",
        "    \"patient_Sex (Male/female)\",\n",
        "    \"fracture_AO/OTA Classification ( 1-6)\",\n",
        "    \"postop_MPTA (1=wrong,0=good)\",\n",
        "    \"postop_PPTA  (1=wrong,0=good)\"\n",
        "]\n",
        "\n",
        "os.makedirs(\"output\", exist_ok=True)\n",
        "outcomes_path = os.path.join(\"output\", \"outcomes.csv\")\n",
        "\n",
        "years = [2, 5]\n",
        "models = ['logistic', 'forest', 'xgboost']\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Helper: Convert original [min, max] into scaled [scaled_min, scaled_max]\n",
        "# ------------------------------------------------------------------------------\n",
        "def get_scaled_bounds(feature_name, original_bounds, trained_scaler, feat_list):\n",
        "    \"\"\"\n",
        "    For a given feature_name, with known 'original_bounds' (e.g. [0, 100]),\n",
        "    returns the scaled_min, scaled_max according to the 'trained_scaler'.\n",
        "\n",
        "    'feat_list' is the list of continuous feature names in the exact order\n",
        "    used when fitting the scaler, so we can retrieve the correct index.\n",
        "    \"\"\"\n",
        "    col_idx = feat_list.index(feature_name)\n",
        "    orig_min, orig_max = original_bounds\n",
        "\n",
        "    mean_ = trained_scaler.mean_[col_idx]\n",
        "    var_  = trained_scaler.var_[col_idx]\n",
        "    std_  = np.sqrt(var_)\n",
        "\n",
        "    scaled_min = (orig_min - mean_) / std_\n",
        "    scaled_max = (orig_max - mean_) / std_\n",
        "\n",
        "    return scaled_min, scaled_max\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# MAIN LOOP\n",
        "# ------------------------------------------------------------------------------\n",
        "for year in years:\n",
        "    target = f\"outcome_Total_Knee_Prothesis_Within_{year}_Yrs\"\n",
        "\n",
        "    # Drop rows with missing target for this year\n",
        "    df_year = df.dropna(subset=[target])\n",
        "    y = df_year[target].astype(bool)\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # (A) Create and save a single imputer + scaler for this year\n",
        "    #     since they are not model-dependent.\n",
        "    # --------------------------------------------------------------------------\n",
        "    X_all_cont = df_year[continuous_features].copy()\n",
        "    X_all_cat  = df_year[categorical_features].copy()\n",
        "    y_all      = y  # Same as above, just for clarity\n",
        "\n",
        "    # Fit final imputer on *all* data for this year\n",
        "    imputer_final_year = KNNImputer(n_neighbors=5)\n",
        "    X_all_cont_imputed_year = imputer_final_year.fit_transform(X_all_cont)\n",
        "\n",
        "    # Fit final scaler on *all* data for this year\n",
        "    scaler_final_year = StandardScaler()\n",
        "    X_all_cont_scaled_year = scaler_final_year.fit_transform(X_all_cont_imputed_year)\n",
        "\n",
        "    # Save them once (not per model)\n",
        "    imputer_path = os.path.join(\"output\", f\"imputer_{year}.sav\")\n",
        "    scaler_path  = os.path.join(\"output\", f\"scaler_{year}.sav\")\n",
        "    pickle.dump(imputer_final_year, open(imputer_path, 'wb'))\n",
        "    pickle.dump(scaler_final_year, open(scaler_path, 'wb'))\n",
        "\n",
        "    # This DataFrame will be used later for training final models.\n",
        "    X_all_cont_scaled_df_year = pd.DataFrame(\n",
        "        X_all_cont_scaled_year,\n",
        "        columns=continuous_features,\n",
        "        index=df_year.index\n",
        "    )\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # (B) Cross-validation + model-specific final training, per model\n",
        "    # --------------------------------------------------------------------------\n",
        "    for model_name in models:\n",
        "        print(f\"\\nModel: {model_name}, Year: {year}\")\n",
        "\n",
        "        # Create a subfolder for each model-year\n",
        "        run_folder = os.path.join(\"output\", f\"{model_name}_{year}\")\n",
        "        os.makedirs(run_folder, exist_ok=True)\n",
        "\n",
        "        # A prefix we'll use for naming all output files\n",
        "        prefix = f\"{model_name}_{year}\"\n",
        "\n",
        "        # Lists for storing metrics across folds\n",
        "        all_f1_scores = []\n",
        "        all_aucs = []\n",
        "        all_precision = []\n",
        "        all_recall = []\n",
        "        all_f1_macro = []\n",
        "        all_precision_macro = []\n",
        "        all_recall_macro = []\n",
        "\n",
        "        cumulative_confusion_matrix = np.zeros((2, 2))\n",
        "        all_probs = []\n",
        "        all_y_test = []\n",
        "        all_y_pred = []\n",
        "\n",
        "        predictions_list_for_model_year = []\n",
        "        feature_importances_list_for_model_year = []\n",
        "\n",
        "        fold_fprs = []\n",
        "        fold_tprs = []\n",
        "        fold_roc_aucs = []\n",
        "\n",
        "        # ===========================================================\n",
        "        # 1) CROSS VALIDATION (LEAVE-ONE-CENTER-OUT) FOR EVALUATION\n",
        "        # ===========================================================\n",
        "        for test_center in unique_centers:\n",
        "            train_mask = (df_year[center_column] != test_center)\n",
        "            test_mask  = (df_year[center_column] == test_center)\n",
        "\n",
        "            X_train_df, X_test_df = df_year.loc[train_mask], df_year.loc[test_mask]\n",
        "            y_train, y_test       = y.loc[train_mask], y.loc[test_mask]\n",
        "\n",
        "            if len(y_test) == 0:\n",
        "                continue\n",
        "\n",
        "            # 1A) Split continuous vs. categorical\n",
        "            X_train_cont = X_train_df[continuous_features].copy()\n",
        "            X_test_cont  = X_test_df[continuous_features].copy()\n",
        "            X_train_cat  = X_train_df[categorical_features].copy()\n",
        "            X_test_cat   = X_test_df[categorical_features].copy()\n",
        "\n",
        "            # 1B) Impute + scale *for this CV fold only*\n",
        "            imputer_fold = KNNImputer(n_neighbors=5)\n",
        "            X_train_cont_imputed = imputer_fold.fit_transform(X_train_cont)\n",
        "            X_test_cont_imputed  = imputer_fold.transform(X_test_cont)\n",
        "\n",
        "            scaler_fold = StandardScaler()\n",
        "            X_train_cont_scaled = scaler_fold.fit_transform(X_train_cont_imputed)\n",
        "            X_test_cont_scaled  = scaler_fold.transform(X_test_cont_imputed)\n",
        "\n",
        "            X_train_cont_scaled_df = pd.DataFrame(\n",
        "                X_train_cont_scaled,\n",
        "                columns=continuous_features,\n",
        "                index=X_train_df.index\n",
        "            )\n",
        "            X_test_cont_scaled_df = pd.DataFrame(\n",
        "                X_test_cont_scaled,\n",
        "                columns=continuous_features,\n",
        "                index=X_test_df.index\n",
        "            )\n",
        "\n",
        "            # 1C) Build final train/test features depending on model\n",
        "            if model_name == 'logistic':\n",
        "                # Create splines with boundary knots (scaled) for each feature\n",
        "                X_spline_list_train = []\n",
        "                X_spline_list_test = []\n",
        "\n",
        "                for cfeat in continuous_features:\n",
        "                    # Skip if not enough data\n",
        "                    cvals_train = X_train_cont_scaled_df[cfeat].dropna()\n",
        "                    if len(cvals_train) < 4:\n",
        "                        print(f\"[Fold CV] Dropping {cfeat} - insufficient data.\")\n",
        "                        continue\n",
        "\n",
        "                    # Retrieve original bounds + convert to scaled space\n",
        "                    bounds_key = feature_bounds_map[cfeat]\n",
        "                    orig_bounds = input_bounds[bounds_key]\n",
        "\n",
        "                    scaled_left, scaled_right = get_scaled_bounds(\n",
        "                        feature_name=cfeat,\n",
        "                        original_bounds=orig_bounds,\n",
        "                        trained_scaler=scaler_fold,\n",
        "                        feat_list=continuous_features\n",
        "                    )\n",
        "\n",
        "                    # Internal knots: 25, 50, 75 percentiles (from training data)\n",
        "                    knots = np.percentile(cvals_train, [25, 50, 75])\n",
        "\n",
        "                    # -- Train set splines --\n",
        "                    train_values = X_train_cont_scaled_df[cfeat].values\n",
        "                    train_bs = bs(\n",
        "                        train_values,\n",
        "                        knots=knots,\n",
        "                        degree=3,\n",
        "                        include_intercept=False,\n",
        "                        lower_bound=scaled_left,\n",
        "                        upper_bound=scaled_right\n",
        "                    )\n",
        "                    spline_col_names = [f\"{cfeat}_spline_{i}\" for i in range(train_bs.shape[1])]\n",
        "                    spline_train = pd.DataFrame(\n",
        "                        train_bs,\n",
        "                        columns=spline_col_names,\n",
        "                        index=X_train_cont_scaled_df.index\n",
        "                    )\n",
        "                    X_spline_list_train.append(spline_train)\n",
        "\n",
        "                    # -- Test set splines --\n",
        "                    test_values = X_test_cont_scaled_df[cfeat].values\n",
        "                    test_bs = bs(\n",
        "                        test_values,\n",
        "                        knots=knots,\n",
        "                        degree=3,\n",
        "                        include_intercept=False,\n",
        "                        lower_bound=scaled_left,\n",
        "                        upper_bound=scaled_right\n",
        "                    )\n",
        "                    spline_test = pd.DataFrame(\n",
        "                        test_bs,\n",
        "                        columns=spline_col_names,\n",
        "                        index=X_test_cont_scaled_df.index\n",
        "                    )\n",
        "                    X_spline_list_test.append(spline_test)\n",
        "\n",
        "                if X_spline_list_train:\n",
        "                    X_spline_all_train = pd.concat(X_spline_list_train, axis=1)\n",
        "                    X_spline_all_test  = pd.concat(X_spline_list_test, axis=1)\n",
        "                else:\n",
        "                    X_spline_all_train = pd.DataFrame(index=X_train_df.index)\n",
        "                    X_spline_all_test  = pd.DataFrame(index=X_test_df.index)\n",
        "\n",
        "                X_train_final = pd.concat([X_train_cat, X_spline_all_train], axis=1)\n",
        "                X_test_final  = pd.concat([X_test_cat,  X_spline_all_test], axis=1)\n",
        "\n",
        "            else:\n",
        "                # forest/xgboost => just scaled + cat (no spline transformations)\n",
        "                X_train_final = pd.concat([X_train_cont_scaled_df, X_train_cat], axis=1)\n",
        "                X_test_final  = pd.concat([X_test_cont_scaled_df,  X_test_cat], axis=1)\n",
        "\n",
        "            # 1D) Choose model\n",
        "            if model_name == 'logistic':\n",
        "                base_model = LogisticRegression()\n",
        "            elif model_name == 'forest':\n",
        "                base_model = RandomForestClassifier()\n",
        "            elif model_name == 'xgboost':\n",
        "                base_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "            # Nested validation for threshold selection\n",
        "            X_train_nested, X_val_nested, y_train_nested, y_val_nested = train_test_split(\n",
        "                X_train_final, y_train, test_size=0.2, random_state=42\n",
        "            )\n",
        "            model_fold = base_model\n",
        "            model_fold.fit(X_train_nested, y_train_nested)\n",
        "\n",
        "            thresholds = np.linspace(0.1, 0.9, 9)\n",
        "            y_proba_val = model_fold.predict_proba(X_val_nested)[:, 1]\n",
        "            best_threshold = 0\n",
        "            best_f1 = 0\n",
        "            for threshold in thresholds:\n",
        "                y_pred_threshold = (y_proba_val >= threshold).astype(int)\n",
        "                f1_tmp = f1_score(y_val_nested, y_pred_threshold)\n",
        "                if f1_tmp > best_f1:\n",
        "                    best_f1 = f1_tmp\n",
        "                    best_threshold = threshold\n",
        "\n",
        "            # Retrain on full training set for this fold\n",
        "            model_fold.fit(X_train_final, y_train)\n",
        "            y_proba = model_fold.predict_proba(X_test_final)[:, 1]\n",
        "            y_pred = (y_proba >= best_threshold).astype(int)\n",
        "\n",
        "            # Metrics\n",
        "            f1       = f1_score(y_test, y_pred)\n",
        "            precision= precision_score(y_test, y_pred)\n",
        "            recall   = recall_score(y_test, y_pred)\n",
        "\n",
        "            f1_macro       = f1_score(y_test, y_pred, average='macro')\n",
        "            precision_macro= precision_score(y_test, y_pred, average='macro')\n",
        "            recall_macro   = recall_score(y_test, y_pred, average='macro')\n",
        "\n",
        "            fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "            roc_auc     = auc(fpr, tpr)\n",
        "\n",
        "            all_f1_scores.append(f1)\n",
        "            all_aucs.append(roc_auc)\n",
        "            all_precision.append(precision)\n",
        "            all_recall.append(recall)\n",
        "            all_f1_macro.append(f1_macro)\n",
        "            all_precision_macro.append(precision_macro)\n",
        "            all_recall_macro.append(recall_macro)\n",
        "\n",
        "            all_probs.extend(y_proba)\n",
        "            all_y_test.extend(y_test)\n",
        "            all_y_pred.extend(y_pred)\n",
        "\n",
        "            fold_confusion_matrix = confusion_matrix(y_test, y_pred)\n",
        "            cumulative_confusion_matrix += fold_confusion_matrix\n",
        "\n",
        "            # Predictions table\n",
        "            df_pred_center = pd.DataFrame({\n",
        "                \"patient_Patient number\": X_test_df[\"patient_Patient number\"],\n",
        "                \"center\": test_center,\n",
        "                \"year\": year,\n",
        "                \"model\": model_name,\n",
        "                \"true_label\": y_test.values,\n",
        "                \"predicted_proba\": y_proba,\n",
        "                \"predicted_label\": y_pred\n",
        "            })\n",
        "            predictions_list_for_model_year.append(df_pred_center)\n",
        "\n",
        "            # Feature importances\n",
        "            if model_name == 'logistic':\n",
        "                importances = model_fold.coef_[0]\n",
        "            else:\n",
        "                importances = model_fold.feature_importances_\n",
        "\n",
        "            final_feature_names = X_train_final.columns.tolist()\n",
        "            df_importance = pd.DataFrame({\n",
        "                'feature': final_feature_names,\n",
        "                'importance': importances\n",
        "            })\n",
        "            df_importance['center'] = test_center\n",
        "            df_importance['model'] = model_name\n",
        "            df_importance['year'] = year\n",
        "\n",
        "            feature_importances_list_for_model_year.append(df_importance)\n",
        "\n",
        "            fold_fprs.append(fpr)\n",
        "            fold_tprs.append(tpr)\n",
        "            fold_roc_aucs.append(roc_auc)\n",
        "\n",
        "        # ===========================================================\n",
        "        # 2) AFTER CROSS-VAL, PLOT AND TRAIN A FINAL MODEL\n",
        "        # ===========================================================\n",
        "        if len(all_f1_scores) > 0:\n",
        "            mean_f1   = np.mean(all_f1_scores)\n",
        "            mean_auc  = np.mean(all_aucs)\n",
        "            mean_prec = np.mean(all_precision)\n",
        "            mean_rec  = np.mean(all_recall)\n",
        "\n",
        "            mean_f1_macro        = np.mean(all_f1_macro)\n",
        "            mean_precision_macro = np.mean(all_precision_macro)\n",
        "            mean_recall_macro    = np.mean(all_recall_macro)\n",
        "\n",
        "            print(f\"Mean F1: {mean_f1:.2f}\")\n",
        "            print(f\"Mean AUC: {mean_auc:.2f}\")\n",
        "            print(f\"Mean Precision: {mean_prec:.2f}\")\n",
        "            print(f\"Mean Recall: {mean_rec:.2f}\")\n",
        "            print(f\"Mean F1 (macro): {mean_f1_macro:.2f}\")\n",
        "            print(f\"Mean Precision (macro): {mean_precision_macro:.2f}\")\n",
        "            print(f\"Mean Recall (macro): {mean_recall_macro:.2f}\")\n",
        "\n",
        "            # Confusion Matrix\n",
        "            cm_filename = os.path.join(run_folder, f\"{prefix}_confusion_matrix.png\")\n",
        "            ConfusionMatrixDisplay(confusion_matrix=cumulative_confusion_matrix).plot(values_format='g')\n",
        "            plt.title(f\"Confusion Matrix: {model_name}, {year}-Year\")\n",
        "            plt.savefig(cm_filename)\n",
        "            plt.show()\n",
        "\n",
        "            # Bootstrapping for AUC\n",
        "            n_bootstraps = 100\n",
        "            rng = np.random.RandomState(42)\n",
        "            boot_aucs = []\n",
        "            all_y_test_arr = np.array(all_y_test)\n",
        "            all_probs_arr  = np.array(all_probs)\n",
        "            for i in range(n_bootstraps):\n",
        "                indices = rng.randint(0, len(all_y_test_arr), len(all_y_test_arr))\n",
        "                # If sample is all one class, skip\n",
        "                if len(np.unique(all_y_test_arr[indices])) < 2:\n",
        "                    continue\n",
        "                fpr_b, tpr_b, _ = roc_curve(all_y_test_arr[indices], all_probs_arr[indices])\n",
        "                roc_auc_b = auc(fpr_b, tpr_b)\n",
        "                boot_aucs.append(roc_auc_b)\n",
        "\n",
        "            if len(boot_aucs) > 0:\n",
        "                lower_ci = np.percentile(boot_aucs, 2.5)\n",
        "                upper_ci = np.percentile(boot_aucs, 97.5)\n",
        "                print(f\"Bootstrap AUC Mean: {np.mean(boot_aucs):.2f}, \"\n",
        "                      f\"95% CI: ({lower_ci:.2f}-{upper_ci:.2f})\")\n",
        "\n",
        "            # Calibration curve\n",
        "            cal_curve_filename = os.path.join(run_folder, f\"{prefix}_calibration_curve.png\")\n",
        "            prob_true, prob_pred = calibration_curve(all_y_test, all_probs, n_bins=10)\n",
        "            plt.figure()\n",
        "            plt.plot(prob_pred, prob_true, marker='o', label='Calibration')\n",
        "            plt.plot([0,1],[0,1], 'k--', label='Perfectly calibrated')\n",
        "            plt.title(f\"Calibration Curve: {model_name}, {year}-Year\")\n",
        "            plt.xlabel('Predicted Probability')\n",
        "            plt.ylabel('Fraction of Positives')\n",
        "            plt.legend()\n",
        "            plt.savefig(cal_curve_filename)\n",
        "            plt.show()\n",
        "\n",
        "            # Summarize crossâ€val run\n",
        "            run_data = {\n",
        "                \"name\": f\"{model_name}_{year}\",\n",
        "                \"auc\": round(mean_auc, 3),\n",
        "                \"f1\": round(mean_f1, 3),\n",
        "                \"recall\": round(mean_rec, 3),\n",
        "                \"precision\": round(mean_prec, 3),\n",
        "                \"f1_macro\": round(mean_f1_macro, 3),\n",
        "                \"recall_macro\": round(mean_recall_macro, 3),\n",
        "                \"precision_macro\": round(mean_precision_macro, 3),\n",
        "                \"bootstrap_auc_mean\": round(np.mean(boot_aucs), 3) if len(boot_aucs) > 0 else np.nan,\n",
        "                \"bootstrap_auc_ci_lower\": round(lower_ci, 3) if len(boot_aucs) > 0 else np.nan,\n",
        "                \"bootstrap_auc_ci_upper\": round(upper_ci, 3) if len(boot_aucs) > 0 else np.nan,\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                outcome = pd.read_csv(outcomes_path)\n",
        "                outcome = pd.concat([outcome, pd.DataFrame.from_dict([run_data])], ignore_index=True)\n",
        "                outcome.to_csv(outcomes_path, index=False)\n",
        "            except FileNotFoundError:\n",
        "                pd.DataFrame.from_dict([run_data]).to_csv(outcomes_path, index=False)\n",
        "\n",
        "            # Save predictions\n",
        "            if predictions_list_for_model_year:\n",
        "                df_all_preds_this_model_year = pd.concat(predictions_list_for_model_year, ignore_index=True)\n",
        "                df_all_preds_this_model_year.to_csv(\n",
        "                    os.path.join(run_folder, f\"{prefix}_predictions.csv\"),\n",
        "                    index=False\n",
        "                )\n",
        "\n",
        "            # Save feature importances\n",
        "            if feature_importances_list_for_model_year:\n",
        "                df_feature_importances = pd.concat(feature_importances_list_for_model_year, ignore_index=True)\n",
        "                df_feature_importances.to_csv(\n",
        "                    os.path.join(run_folder, f\"{prefix}_feature_importances.csv\"),\n",
        "                    index=False\n",
        "                )\n",
        "\n",
        "            # ============================================\n",
        "            # 3) TRAIN THE FINAL MODEL ON ALL df_year DATA\n",
        "            #    (using the year-level imputer/scaler)\n",
        "            # ============================================\n",
        "            print(f\"Training final {model_name} model on *all* data (year={year})...\")\n",
        "\n",
        "            # We already have X_all_cont_scaled_df_year, X_all_cat, y_all\n",
        "            # from the year-level imputer/scaler\n",
        "            knots_dict_final = {}\n",
        "            if model_name == 'logistic':\n",
        "                # Build spline columns using the already-scaled data\n",
        "                X_spline_list_all = []\n",
        "                for cfeat in continuous_features:\n",
        "                    cvals_all = X_all_cont_scaled_df_year[cfeat].dropna()\n",
        "                    if len(cvals_all) < 4:\n",
        "                        print(f\"(Final) Dropping {cfeat} due to insufficient data.\")\n",
        "                        continue\n",
        "\n",
        "                    # Retrieve original [min, max] -> scaled\n",
        "                    bounds_key = feature_bounds_map[cfeat]\n",
        "                    b_left_orig, b_right_orig = input_bounds[bounds_key]\n",
        "                    scaled_left_final, scaled_right_final = get_scaled_bounds(\n",
        "                        feature_name=cfeat,\n",
        "                        original_bounds=[b_left_orig, b_right_orig],\n",
        "                        trained_scaler=scaler_final_year,\n",
        "                        feat_list=continuous_features\n",
        "                    )\n",
        "\n",
        "                    # Internal knots from entire data distribution\n",
        "                    knots_final = np.percentile(cvals_all, [25, 50, 75])\n",
        "\n",
        "                    # Save them for production usage\n",
        "                    knots_dict_final[cfeat] = {\n",
        "                        \"internal_knots\": knots_final.tolist(),\n",
        "                        \"boundary_knots_original\": [float(b_left_orig), float(b_right_orig)],\n",
        "                        \"boundary_knots_scaled\": [float(scaled_left_final), float(scaled_right_final)]\n",
        "                    }\n",
        "\n",
        "                    all_values = X_all_cont_scaled_df_year[cfeat].values\n",
        "                    spline_all = bs(\n",
        "                        all_values,\n",
        "                        knots=knots_final,\n",
        "                        degree=3,\n",
        "                        include_intercept=False,\n",
        "                        lower_bound=scaled_left_final,\n",
        "                        upper_bound=scaled_right_final\n",
        "                    )\n",
        "                    spline_col_names = [f\"{cfeat}_spline_{i}\" for i in range(spline_all.shape[1])]\n",
        "                    spline_all_df = pd.DataFrame(\n",
        "                        spline_all,\n",
        "                        columns=spline_col_names,\n",
        "                        index=X_all_cont_scaled_df_year.index\n",
        "                    )\n",
        "                    X_spline_list_all.append(spline_all_df)\n",
        "\n",
        "                if X_spline_list_all:\n",
        "                    X_spline_all_final = pd.concat(X_spline_list_all, axis=1)\n",
        "                else:\n",
        "                    X_spline_all_final = pd.DataFrame(index=df_year.index)\n",
        "\n",
        "                X_final_all = pd.concat([X_all_cat, X_spline_all_final], axis=1)\n",
        "                final_model = LogisticRegression()\n",
        "\n",
        "            elif model_name == 'forest':\n",
        "                # Random Forest uses scaled continuous + cat (no spline)\n",
        "                X_final_all = pd.concat([X_all_cont_scaled_df_year, X_all_cat], axis=1)\n",
        "                final_model = RandomForestClassifier()\n",
        "\n",
        "            elif model_name == 'xgboost':\n",
        "                # XGBoost uses scaled continuous + cat (no spline)\n",
        "                X_final_all = pd.concat([X_all_cont_scaled_df_year, X_all_cat], axis=1)\n",
        "                final_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "\n",
        "            print(\"X_final_all shape:\", X_final_all.shape)\n",
        "            print(\"y_all shape:\", y_all.shape)\n",
        "\n",
        "            # Fit final model\n",
        "            final_model.fit(X_final_all, y_all)\n",
        "\n",
        "            # Save final model (named as <model>_<year>.sav)\n",
        "            model_filename = os.path.join(run_folder, f\"{prefix}.sav\")\n",
        "            pickle.dump(final_model, open(model_filename, 'wb'))\n",
        "\n",
        "            # Save the final spline boundary/knots data (only relevant for logistic)\n",
        "            # For forest/xgboost, we keep it empty or skip.\n",
        "            knots_path = os.path.join(run_folder, f\"{prefix}_knots.json\")\n",
        "            with open(knots_path, \"w\") as f:\n",
        "                json.dump(knots_dict_final, f, indent=2)\n",
        "\n",
        "            print(f\"Saved final {model_name} for {year}-year to {run_folder}.\\n\")\n",
        "        # end if cross-val had folds\n",
        "    # end model loop\n",
        "# end year loop\n"
      ],
      "metadata": {
        "id": "0fwHQEmS9c_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Output (Google Colab)"
      ],
      "metadata": {
        "id": "PtVOqmSssU_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Check if running in Google Colab\n",
        "    import google.colab\n",
        "\n",
        "    # 1. Zip a specific folder (replace 'output' with your folder name).\n",
        "    !zip -r output.zip output\n",
        "\n",
        "    # 2. Download the zip file.\n",
        "    from google.colab import files\n",
        "    files.download('output.zip')\n",
        "\n",
        "except ImportError:\n",
        "    print(\"This script is intended to run on Google Colab.\")\n"
      ],
      "metadata": {
        "id": "Ks4xI6xBsYAL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}